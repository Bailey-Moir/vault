---
tags:
  - comp
---
Language models = machine learning models that can predict relations between words and sequences of words, e.g. an 'n-gram model'
> [!examples]
> 1. "Mary rowed the boat towards the \[MASK\]"
> 1. "Mary rowed the \[MASK\] upstream"
> 1. "Mary pushed the \[MASK\] upstream"

They are used for
- Question answering
- Semantic role labelling
- Named entity recognition
- Sentiment analysis
- Machine translation
- Text generation
# Transformer Models
Introduced 'attention', i.e. how to weigh words that should have more influence in predicting the masked word?